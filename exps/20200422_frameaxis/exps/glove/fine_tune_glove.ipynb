{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "import string\n",
    "string.punctuation += '➡•’…'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_df = pickle.load(open('../../../../data/all_tweet_texts_dem.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_df = pickle.load(open('../../../../data/all_tweet_texts_rep.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(134337, 85456)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dem_df), len(rep_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_text = dem_df.Text.tolist()\n",
    "rep_text = rep_df.Text.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = dem_text + rep_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"We had our own bad weather &amp; power outages here today, but we're thinking of our friends down south &amp; who were hit hard by devastating tornadoes &amp; extreme weather. We are with you in spirit, we will pitch in to help you recover, &amp; we are all #OneNation.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # without processing\n",
    "# with open('glove_corpus_basic.txt', 'w') as g:\n",
    "#     for line in text:\n",
    "#         g.write(line)\n",
    "#         g.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = open('vocab_basic.txt', 'r').readlines()\n",
    "# covid_vocab = []\n",
    "# for line in vocab:\n",
    "#     line = line.strip().lower()\n",
    "#     if 'covid' in line or 'corona' in line:\n",
    "#         covid_vocab.append(line.split(' ')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(covid_vocab, open('../../../../data/all_covid_words.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_vocab = pickle.load(open('../../../../data/all_covid_words.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# covid_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_regex = re.compile('|'.join(map(re.escape, covid_vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_text = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in text:\n",
    "    line = line.lower()\n",
    "    processed_line = big_regex.sub('covid', line)\n",
    "    processed_text.append(processed_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "219793"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load('en_core_web_md', disable=['parser', 'tagger', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"we had our own bad weather &amp; power outages here today, but we're thinking of our friends down south &amp; who were hit hard by devastating tornadoes &amp; extreme weather. we are with you in spirit, we will pitch in to help you recover, &amp; we are all #onenation.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~➡•’…'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = []\n",
    "for line in processed_text:\n",
    "    line = line.replace('&amp;', '&')\n",
    "    line = line.translate(str.maketrans('', '', string.punctuation)).replace('\\n', '') \n",
    "    tokenized_line = [token.text for token in nlp(line)]\n",
    "    tokenized_text.append(' '.join(tokenized_line))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = \"We're grateful to so many essential workers whose jobs have taken on added #covidpandemic risk during this pandemic.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# big_regex.sub('covid', sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we had our own bad weather   power outages here today but were thinking of our friends down south   who were hit hard by devastating tornadoes   extreme weather we are with you in spirit we will pitch in to help you recover   we are all onenation\n",
      "rt agneronha questions about scams price gouging background checks public meetings   records requestsvisit our resource page for in\n",
      "ris agneronha is leading efforts to stop scammers if you see a covidscam or think you have been the victim of a scam you may contact the ags consumer protection unit at 401 2744400 or e mail consumersriagrigov\n",
      "help stop covid scammers if anyone calls or texts you claiming to the be the irs and asks for your personal information regarding your covid direct payment or rebate check do not reply they are not legit\n",
      "were grateful to so many essential workers whose jobs have taken on added risk during this pandemic we should do more than applaud we must ensure they are fairly compensated heroesfund hazard pay would give a real pay bump to frontlineworkers\n"
     ]
    }
   ],
   "source": [
    "for line in tokenized_text[0:5]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('glove_new_corpus.txt', 'w') as g:\n",
    "    for line in tokenized_text:\n",
    "        g.write(line)\n",
    "        g.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse fine tuned vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import get_tmpfile, datapath\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28696, 300)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove2word2vec('vectors_50.txt', './vectors_gensim_word2vec_50_epochs.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a KeyedVectors from a temporary file\n",
    "model = KeyedVectors.load_word2vec_format('vectors_gensim_word2vec_50_epochs.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('pandemic', 0.6540495157241821),\n",
       " ('outbreak', 0.6431459188461304),\n",
       " ('response', 0.6113077402114868),\n",
       " ('spread', 0.5741133093833923),\n",
       " ('virus', 0.5205848217010498),\n",
       " ('crisis', 0.5145406723022461),\n",
       " ('now', 0.49959105253219604),\n",
       " ('help', 0.4982158839702606),\n",
       " ('the', 0.4952136278152466),\n",
       " ('health', 0.4913753271102905)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('covid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_vec = open('glove.6B.300d.txt', 'r').readlines()\n",
    "original_vec = [line.strip() for line in original_vec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(original_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_vec = open('vectors_50.txt', 'r').readlines()\n",
    "trained_vec = [line.strip() for line in trained_vec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_vocab = set([line.split()[0] for line in trained_vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('all_glove_50_epoch.txt', 'w') as g:\n",
    "    for line in trained_vec:\n",
    "        if len(line.split()) == 301:\n",
    "            g.write(line)\n",
    "            g.write('\\n')\n",
    "    for line in original_vec:\n",
    "        key = line.split()[0]\n",
    "        if key not in trained_vocab:\n",
    "            g.write(line)\n",
    "            g.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(409003, 300)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove2word2vec('all_glove_50_epoch.txt', './all_glove_gensim_word2vec_50_epoch.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
